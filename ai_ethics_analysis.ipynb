{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b97f164f",
   "metadata": {},
   "source": [
    "# AI Ethics Assignment - Practical Audit\n",
    "## COMPAS Recidivism Dataset Bias Analysis\n",
    "\n",
    "### Objective\n",
    "This notebook analyzes the COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset to identify and measure racial bias in risk assessment scores used in the criminal justice system.\n",
    "\n",
    "### Tools Used\n",
    "- **AI Fairness 360 (AIF360)**: IBM's comprehensive toolkit for bias detection and mitigation\n",
    "- **Pandas**: Data manipulation and analysis\n",
    "- **Matplotlib/Seaborn**: Data visualization\n",
    "- **NumPy**: Numerical computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "# AI Fairness 360 imports\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Starting COMPAS dataset bias analysis...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91297c47",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e6f665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load COMPAS dataset using AIF360\n",
    "try:\n",
    "    # Load the dataset\n",
    "    dataset = CompasDataset()\n",
    "    print(\"COMPAS dataset loaded successfully!\")\n",
    "    print(f\"Dataset shape: {dataset.features.shape}\")\n",
    "    print(f\"Protected attributes: {dataset.protected_attribute_names}\")\n",
    "    print(f\"Label names: {dataset.label_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading COMPAS dataset: {e}\")\n",
    "    print(\"Attempting to load from alternative source...\")\n",
    "    \n",
    "    # Alternative: Load from ProPublica GitHub if AIF360 dataset fails\n",
    "    url = \"https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\"\n",
    "    df = pd.read_csv(url)\n",
    "    print(f\"Alternative dataset loaded with shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec2b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert AIF360 dataset to pandas DataFrame for easier manipulation\n",
    "if 'dataset' in locals():\n",
    "    df = dataset.convert_to_dataframe()[0]\n",
    "    print(\"Dataset converted to pandas DataFrame\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"\\n=== Dataset Overview ===\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75d4e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the distribution of key variables\n",
    "print(\"=== Key Variable Distributions ===\")\n",
    "\n",
    "# Race distribution\n",
    "if 'race' in df.columns:\n",
    "    print(\"\\nRace Distribution:\")\n",
    "    print(df['race'].value_counts())\n",
    "    print(f\"Percentage distribution:\\n{df['race'].value_counts(normalize=True) * 100}\")\n",
    "\n",
    "# Two-year recidivism distribution\n",
    "if 'two_year_recid' in df.columns:\n",
    "    print(\"\\nTwo-Year Recidivism Distribution:\")\n",
    "    print(df['two_year_recid'].value_counts())\n",
    "    print(f\"Recidivism rate: {df['two_year_recid'].mean():.2%}\")\n",
    "\n",
    "# COMPAS scores distribution\n",
    "if 'decile_score' in df.columns:\n",
    "    print(\"\\nCOMPAS Decile Score Statistics:\")\n",
    "    print(df['decile_score'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6fcaf1",
   "metadata": {},
   "source": [
    "## 2. Bias Detection Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf397eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations for bias detection\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('COMPAS Dataset Bias Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Race distribution\n",
    "if 'race' in df.columns:\n",
    "    race_counts = df['race'].value_counts()\n",
    "    axes[0, 0].pie(race_counts.values, labels=race_counts.index, autopct='%1.1f%%')\n",
    "    axes[0, 0].set_title('Distribution by Race')\n",
    "\n",
    "# 2. Recidivism by race\n",
    "if 'race' in df.columns and 'two_year_recid' in df.columns:\n",
    "    recid_by_race = df.groupby('race')['two_year_recid'].mean()\n",
    "    axes[0, 1].bar(recid_by_race.index, recid_by_race.values)\n",
    "    axes[0, 1].set_title('Recidivism Rate by Race')\n",
    "    axes[0, 1].set_ylabel('Recidivism Rate')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. COMPAS scores by race\n",
    "if 'race' in df.columns and 'decile_score' in df.columns:\n",
    "    sns.boxplot(data=df, x='race', y='decile_score', ax=axes[1, 0])\n",
    "    axes[1, 0].set_title('COMPAS Scores by Race')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Score distribution\n",
    "if 'decile_score' in df.columns:\n",
    "    axes[1, 1].hist(df['decile_score'], bins=10, edgecolor='black')\n",
    "    axes[1, 1].set_title('Distribution of COMPAS Scores')\n",
    "    axes[1, 1].set_xlabel('Decile Score')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/compas_bias_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3258de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize fairness metrics\n",
    "def calculate_fairness_metrics(df, protected_attr='race', target='two_year_recid', score='decile_score'):\n",
    "    \"\"\"\n",
    "    Calculate key fairness metrics for bias analysis\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    \n",
    "    # Define high risk threshold (scores 7-10 typically considered high risk)\n",
    "    high_risk_threshold = 7\n",
    "    df['high_risk'] = (df[score] >= high_risk_threshold).astype(int)\n",
    "    \n",
    "    # Get unique groups\n",
    "    groups = df[protected_attr].unique()\n",
    "    \n",
    "    for group in groups:\n",
    "        group_data = df[df[protected_attr] == group]\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_count = len(group_data)\n",
    "        recid_rate = group_data[target].mean()\n",
    "        high_risk_rate = group_data['high_risk'].mean()\n",
    "        avg_score = group_data[score].mean()\n",
    "        \n",
    "        # Confusion matrix elements for fairness metrics\n",
    "        tp = len(group_data[(group_data['high_risk'] == 1) & (group_data[target] == 1)])\n",
    "        fp = len(group_data[(group_data['high_risk'] == 1) & (group_data[target] == 0)])\n",
    "        tn = len(group_data[(group_data['high_risk'] == 0) & (group_data[target] == 0)])\n",
    "        fn = len(group_data[(group_data['high_risk'] == 0) & (group_data[target] == 1)])\n",
    "        \n",
    "        # Calculate rates\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate (Sensitivity)\n",
    "        \n",
    "        metrics[group] = {\n",
    "            'count': total_count,\n",
    "            'recidivism_rate': recid_rate,\n",
    "            'high_risk_rate': high_risk_rate,\n",
    "            'avg_score': avg_score,\n",
    "            'false_positive_rate': fpr,\n",
    "            'false_negative_rate': fnr,\n",
    "            'true_positive_rate': tpr\n",
    "        }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Calculate metrics\n",
    "if 'race' in df.columns:\n",
    "    fairness_metrics = calculate_fairness_metrics(df)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=== Fairness Metrics by Race ===\")\n",
    "    metrics_df = pd.DataFrame(fairness_metrics).T\n",
    "    print(metrics_df.round(3))\n",
    "    \n",
    "    # Save to CSV\n",
    "    metrics_df.to_csv('results/fairness_metrics.csv')\n",
    "    print(\"\\nMetrics saved to results/fairness_metrics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf41cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize disparate impact and fairness metrics\n",
    "if 'race' in df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Fairness Metrics Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # False Positive Rates by Race\n",
    "    fpr_data = {race: metrics['false_positive_rate'] for race, metrics in fairness_metrics.items()}\n",
    "    axes[0, 0].bar(fpr_data.keys(), fpr_data.values(), color='coral')\n",
    "    axes[0, 0].set_title('False Positive Rate by Race')\n",
    "    axes[0, 0].set_ylabel('False Positive Rate')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # False Negative Rates by Race\n",
    "    fnr_data = {race: metrics['false_negative_rate'] for race, metrics in fairness_metrics.items()}\n",
    "    axes[0, 1].bar(fnr_data.keys(), fnr_data.values(), color='lightblue')\n",
    "    axes[0, 1].set_title('False Negative Rate by Race')\n",
    "    axes[0, 1].set_ylabel('False Negative Rate')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # High Risk Classification Rate by Race\n",
    "    hr_data = {race: metrics['high_risk_rate'] for race, metrics in fairness_metrics.items()}\n",
    "    axes[1, 0].bar(hr_data.keys(), hr_data.values(), color='lightgreen')\n",
    "    axes[1, 0].set_title('High Risk Classification Rate by Race')\n",
    "    axes[1, 0].set_ylabel('High Risk Rate')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Average COMPAS Score by Race\n",
    "    score_data = {race: metrics['avg_score'] for race, metrics in fairness_metrics.items()}\n",
    "    axes[1, 1].bar(score_data.keys(), score_data.values(), color='gold')\n",
    "    axes[1, 1].set_title('Average COMPAS Score by Race')\n",
    "    axes[1, 1].set_ylabel('Average Score')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/fairness_metrics_visualization.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889db96",
   "metadata": {},
   "source": [
    "## 3. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813c5afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Perform statistical tests to determine if observed differences are significant\n",
    "def perform_statistical_tests(df, protected_attr='race'):\n",
    "    \"\"\"\n",
    "    Perform statistical tests to assess significance of observed disparities\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Get the two largest racial groups for comparison\n",
    "    race_counts = df[protected_attr].value_counts()\n",
    "    top_races = race_counts.head(2).index.tolist()\n",
    "    \n",
    "    group1_data = df[df[protected_attr] == top_races[0]]\n",
    "    group2_data = df[df[protected_attr] == top_races[1]]\n",
    "    \n",
    "    print(f\"Comparing {top_races[0]} (n={len(group1_data)}) vs {top_races[1]} (n={len(group2_data)})\")\n",
    "    \n",
    "    # Test 1: Difference in COMPAS scores\n",
    "    if 'decile_score' in df.columns:\n",
    "        score_stat, score_p = stats.mannwhitneyu(\n",
    "            group1_data['decile_score'], \n",
    "            group2_data['decile_score'], \n",
    "            alternative='two-sided'\n",
    "        )\n",
    "        results['score_difference'] = {\n",
    "            'statistic': score_stat,\n",
    "            'p_value': score_p,\n",
    "            'significant': score_p < 0.05\n",
    "        }\n",
    "    \n",
    "    # Test 2: Difference in recidivism rates\n",
    "    if 'two_year_recid' in df.columns:\n",
    "        recid_stat, recid_p = stats.chi2_contingency([\n",
    "            [group1_data['two_year_recid'].sum(), len(group1_data) - group1_data['two_year_recid'].sum()],\n",
    "            [group2_data['two_year_recid'].sum(), len(group2_data) - group2_data['two_year_recid'].sum()]\n",
    "        ])[0:2]\n",
    "        results['recidivism_difference'] = {\n",
    "            'statistic': recid_stat,\n",
    "            'p_value': recid_p,\n",
    "            'significant': recid_p < 0.05\n",
    "        }\n",
    "    \n",
    "    return results, top_races\n",
    "\n",
    "if 'race' in df.columns:\n",
    "    test_results, compared_races = perform_statistical_tests(df)\n",
    "    \n",
    "    print(\"\\n=== Statistical Test Results ===\")\n",
    "    for test_name, result in test_results.items():\n",
    "        print(f\"\\n{test_name.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Statistic: {result['statistic']:.4f}\")\n",
    "        print(f\"  P-value: {result['p_value']:.6f}\")\n",
    "        print(f\"  Significant: {'Yes' if result['significant'] else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f135955a",
   "metadata": {},
   "source": [
    "## 4. Disparate Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4deba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate disparate impact ratios\n",
    "def calculate_disparate_impact(df, protected_attr='race', outcome='high_risk'):\n",
    "    \"\"\"\n",
    "    Calculate disparate impact ratios using the 80% rule\n",
    "    \"\"\"\n",
    "    # Ensure high_risk column exists\n",
    "    if 'high_risk' not in df.columns and 'decile_score' in df.columns:\n",
    "        df['high_risk'] = (df['decile_score'] >= 7).astype(int)\n",
    "    \n",
    "    rates = df.groupby(protected_attr)[outcome].mean()\n",
    "    \n",
    "    # Use the group with the highest rate as reference\n",
    "    max_rate = rates.max()\n",
    "    max_group = rates.idxmax()\n",
    "    \n",
    "    disparate_impact = {}\n",
    "    for group, rate in rates.items():\n",
    "        di_ratio = rate / max_rate if max_rate > 0 else 0\n",
    "        disparate_impact[group] = {\n",
    "            'rate': rate,\n",
    "            'di_ratio': di_ratio,\n",
    "            'passes_80_rule': di_ratio >= 0.8\n",
    "        }\n",
    "    \n",
    "    return disparate_impact, max_group\n",
    "\n",
    "if 'race' in df.columns:\n",
    "    di_results, reference_group = calculate_disparate_impact(df)\n",
    "    \n",
    "    print(f\"=== Disparate Impact Analysis (Reference: {reference_group}) ===\")\n",
    "    print(\"\\nDisparate Impact Ratios for High-Risk Classification:\")\n",
    "    \n",
    "    di_df = pd.DataFrame(di_results).T\n",
    "    print(di_df.round(3))\n",
    "    \n",
    "    # Visualize disparate impact\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: High-risk rates by race\n",
    "    rates = [result['rate'] for result in di_results.values()]\n",
    "    groups = list(di_results.keys())\n",
    "    colors = ['red' if not di_results[group]['passes_80_rule'] else 'green' for group in groups]\n",
    "    \n",
    "    ax1.bar(groups, rates, color=colors, alpha=0.7)\n",
    "    ax1.set_title('High-Risk Classification Rates by Race')\n",
    "    ax1.set_ylabel('High-Risk Rate')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Add 80% rule line\n",
    "    max_rate_val = max(rates)\n",
    "    ax1.axhline(y=max_rate_val * 0.8, color='orange', linestyle='--', label='80% Rule Threshold')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot 2: Disparate impact ratios\n",
    "    di_ratios = [result['di_ratio'] for result in di_results.values()]\n",
    "    ax2.bar(groups, di_ratios, color=colors, alpha=0.7)\n",
    "    ax2.set_title('Disparate Impact Ratios')\n",
    "    ax2.set_ylabel('DI Ratio')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.axhline(y=0.8, color='orange', linestyle='--', label='80% Rule')\n",
    "    ax2.axhline(y=1.0, color='blue', linestyle='-', alpha=0.5, label='Parity')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/disparate_impact_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary\n",
    "    failing_groups = [group for group, result in di_results.items() if not result['passes_80_rule']]\n",
    "    if failing_groups:\n",
    "        print(f\"\\nâš ï¸  Groups failing the 80% rule: {failing_groups}\")\n",
    "    else:\n",
    "        print(\"\\nâœ… All groups pass the 80% rule for disparate impact.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b2a985",
   "metadata": {},
   "source": [
    "## 5. Predictive Bias Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3322a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze predictive performance across racial groups\n",
    "def analyze_predictive_bias(df, protected_attr='race', score_col='decile_score', target_col='two_year_recid'):\n",
    "    \"\"\"\n",
    "    Analyze how well COMPAS scores predict actual recidivism across different groups\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import roc_auc_score, precision_score, recall_score\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Convert scores to binary predictions (high risk = score >= 7)\n",
    "    df['predicted_high_risk'] = (df[score_col] >= 7).astype(int)\n",
    "    \n",
    "    for group in df[protected_attr].unique():\n",
    "        group_data = df[df[protected_attr] == group]\n",
    "        \n",
    "        if len(group_data) > 10:  # Ensure sufficient sample size\n",
    "            # Calculate predictive metrics\n",
    "            auc = roc_auc_score(group_data[target_col], group_data[score_col])\n",
    "            precision = precision_score(group_data[target_col], group_data['predicted_high_risk'])\n",
    "            recall = recall_score(group_data[target_col], group_data['predicted_high_risk'])\n",
    "            \n",
    "            # Calculate calibration (what % of high-risk predictions actually recidivate)\n",
    "            high_risk_subset = group_data[group_data['predicted_high_risk'] == 1]\n",
    "            calibration = high_risk_subset[target_col].mean() if len(high_risk_subset) > 0 else 0\n",
    "            \n",
    "            results[group] = {\n",
    "                'sample_size': len(group_data),\n",
    "                'auc_score': auc,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'calibration': calibration\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "if 'race' in df.columns and 'decile_score' in df.columns and 'two_year_recid' in df.columns:\n",
    "    predictive_results = analyze_predictive_bias(df)\n",
    "    \n",
    "    print(\"=== Predictive Performance by Race ===\")\n",
    "    pred_df = pd.DataFrame(predictive_results).T\n",
    "    print(pred_df.round(3))\n",
    "    \n",
    "    # Visualize predictive performance\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Predictive Performance Analysis by Race', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    groups = list(predictive_results.keys())\n",
    "    \n",
    "    # AUC scores\n",
    "    auc_scores = [predictive_results[group]['auc_score'] for group in groups]\n",
    "    axes[0, 0].bar(groups, auc_scores, color='skyblue')\n",
    "    axes[0, 0].set_title('AUC Scores by Race')\n",
    "    axes[0, 0].set_ylabel('AUC Score')\n",
    "    axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "    \n",
    "    # Precision\n",
    "    precision_scores = [predictive_results[group]['precision'] for group in groups]\n",
    "    axes[0, 1].bar(groups, precision_scores, color='lightgreen')\n",
    "    axes[0, 1].set_title('Precision by Race')\n",
    "    axes[0, 1].set_ylabel('Precision')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Recall\n",
    "    recall_scores = [predictive_results[group]['recall'] for group in groups]\n",
    "    axes[1, 0].bar(groups, recall_scores, color='coral')\n",
    "    axes[1, 0].set_title('Recall by Race')\n",
    "    axes[1, 0].set_ylabel('Recall')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Calibration\n",
    "    calibration_scores = [predictive_results[group]['calibration'] for group in groups]\n",
    "    axes[1, 1].bar(groups, calibration_scores, color='gold')\n",
    "    axes[1, 1].set_title('Calibration by Race')\n",
    "    axes[1, 1].set_ylabel('Positive Predictive Value')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results/predictive_performance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ae2dfc",
   "metadata": {},
   "source": [
    "## 6. Summary Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abe9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive bias audit report\n",
    "def generate_audit_report(df, fairness_metrics, di_results, predictive_results, test_results):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive bias audit report\n",
    "    \"\"\"\n",
    "    report = []\n",
    "    report.append(\"# COMPAS Dataset Bias Audit Report\")\n",
    "    report.append(\"## Executive Summary\")\n",
    "    \n",
    "    # Dataset overview\n",
    "    total_samples = len(df)\n",
    "    racial_groups = df['race'].nunique() if 'race' in df.columns else 0\n",
    "    overall_recid_rate = df['two_year_recid'].mean() if 'two_year_recid' in df.columns else 0\n",
    "    \n",
    "    report.append(f\"\\n**Dataset Overview:**\")\n",
    "    report.append(f\"- Total samples analyzed: {total_samples:,}\")\n",
    "    report.append(f\"- Racial groups represented: {racial_groups}\")\n",
    "    report.append(f\"- Overall recidivism rate: {overall_recid_rate:.1%}\")\n",
    "    \n",
    "    # Key findings\n",
    "    report.append(f\"\\n## Key Findings\")\n",
    "    \n",
    "    # Disparate impact findings\n",
    "    failing_groups = [group for group, result in di_results.items() if not result['passes_80_rule']]\n",
    "    if failing_groups:\n",
    "        report.append(f\"\\n**âš ï¸ Disparate Impact Detected:**\")\n",
    "        report.append(f\"- Groups failing 80% rule: {', '.join(failing_groups)}\")\n",
    "        for group in failing_groups:\n",
    "            ratio = di_results[group]['di_ratio']\n",
    "            report.append(f\"  - {group}: {ratio:.2f} ratio ({(1-ratio)*100:.1f}% below parity)\")\n",
    "    else:\n",
    "        report.append(f\"\\n**âœ… Disparate Impact Analysis:** All groups pass the 80% rule.\")\n",
    "    \n",
    "    # False positive rate analysis\n",
    "    fpr_values = [metrics['false_positive_rate'] for metrics in fairness_metrics.values()]\n",
    "    fpr_range = max(fpr_values) - min(fpr_values)\n",
    "    report.append(f\"\\n**False Positive Rate Analysis:**\")\n",
    "    report.append(f\"- Range across groups: {fpr_range:.3f}\")\n",
    "    if fpr_range > 0.1:  # 10% difference threshold\n",
    "        report.append(f\"- âš ï¸ Significant disparity detected (>{10}% difference)\")\n",
    "    \n",
    "    # Predictive performance\n",
    "    if predictive_results:\n",
    "        auc_values = [result['auc_score'] for result in predictive_results.values()]\n",
    "        auc_range = max(auc_values) - min(auc_values)\n",
    "        report.append(f\"\\n**Predictive Performance:**\")\n",
    "        report.append(f\"- AUC score range: {min(auc_values):.3f} - {max(auc_values):.3f}\")\n",
    "        if auc_range > 0.05:  # 5% AUC difference threshold\n",
    "            report.append(f\"- âš ï¸ Significant performance disparity detected\")\n",
    "    \n",
    "    # Recommendations\n",
    "    report.append(f\"\\n## Recommendations\")\n",
    "    \n",
    "    if failing_groups or fpr_range > 0.1:\n",
    "        report.append(f\"\\n**Immediate Actions Required:**\")\n",
    "        report.append(f\"1. **Algorithmic Intervention:** Implement fairness constraints or post-processing\")\n",
    "        report.append(f\"2. **Data Audit:** Review training data for historical bias patterns\")\n",
    "        report.append(f\"3. **Process Review:** Establish human oversight for high-risk classifications\")\n",
    "        report.append(f\"4. **Regular Monitoring:** Implement ongoing bias monitoring and reporting\")\n",
    "    \n",
    "    report.append(f\"\\n**Long-term Improvements:**\")\n",
    "    report.append(f\"1. **Diverse Training Data:** Ensure representative samples across all groups\")\n",
    "    report.append(f\"2. **Fairness-aware ML:** Implement algorithms designed for equitable outcomes\")\n",
    "    report.append(f\"3. **Stakeholder Engagement:** Include affected communities in system design\")\n",
    "    report.append(f\"4. **Transparency Measures:** Provide clear explanations for high-risk classifications\")\n",
    "    \n",
    "    # Technical details\n",
    "    report.append(f\"\\n## Technical Details\")\n",
    "    report.append(f\"\\n**Methodology:**\")\n",
    "    report.append(f\"- Fairness metrics calculated using established frameworks\")\n",
    "    report.append(f\"- Statistical significance tested using appropriate tests\")\n",
    "    report.append(f\"- High-risk threshold set at decile score â‰¥ 7\")\n",
    "    report.append(f\"- Analysis conducted using AI Fairness 360 toolkit\")\n",
    "    \n",
    "    return \"\\n\".join(report)\n",
    "\n",
    "# Generate and save the report\n",
    "if 'race' in df.columns:\n",
    "    audit_report = generate_audit_report(df, fairness_metrics, di_results, predictive_results, test_results)\n",
    "    \n",
    "    # Save report\n",
    "    with open('results/bias_audit_report.md', 'w') as f:\n",
    "        f.write(audit_report)\n",
    "    \n",
    "    print(\"=== BIAS AUDIT REPORT ===\")\n",
    "    print(audit_report)\n",
    "    print(\"\\nðŸ“„ Full report saved to: results/bias_audit_report.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cd3d4e",
   "metadata": {},
   "source": [
    "## 7. Mitigation Strategies Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711c486e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate bias mitigation using AI Fairness 360\n",
    "try:\n",
    "    from aif360.algorithms.preprocessing import Reweighing\n",
    "    from aif360.algorithms.postprocessing import CalibratedEqOddsPostprocessing\n",
    "    \n",
    "    print(\"=== Bias Mitigation Demonstration ===\")\n",
    "    \n",
    "    # Convert back to AIF360 format for mitigation\n",
    "    if 'dataset' in locals():\n",
    "        # Define privileged and unprivileged groups\n",
    "        privileged_groups = [{'race': 1.0}]  # Typically white/Caucasian\n",
    "        unprivileged_groups = [{'race': 0.0}]  # Typically African-American\n",
    "        \n",
    "        # Apply reweighing preprocessing\n",
    "        RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                       privileged_groups=privileged_groups)\n",
    "        dataset_transf = RW.fit_transform(dataset)\n",
    "        \n",
    "        # Calculate metrics before and after\n",
    "        metric_orig = BinaryLabelDatasetMetric(dataset, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "        \n",
    "        metric_transf = BinaryLabelDatasetMetric(dataset_transf, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "        \n",
    "        print(f\"\\n**Before Mitigation:**\")\n",
    "        print(f\"Mean difference: {metric_orig.mean_difference():.4f}\")\n",
    "        print(f\"Disparate impact: {metric_orig.disparate_impact():.4f}\")\n",
    "        \n",
    "        print(f\"\\n**After Reweighing:**\")\n",
    "        print(f\"Mean difference: {metric_transf.mean_difference():.4f}\")\n",
    "        print(f\"Disparate impact: {metric_transf.disparate_impact():.4f}\")\n",
    "        \n",
    "        # Visualization of mitigation effect\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Before mitigation\n",
    "        orig_df = dataset.convert_to_dataframe()[0]\n",
    "        pos_rates_orig = orig_df.groupby('race')['two_year_recid'].mean()\n",
    "        ax1.bar(pos_rates_orig.index, pos_rates_orig.values, color='red', alpha=0.7)\n",
    "        ax1.set_title('Before Mitigation')\n",
    "        ax1.set_ylabel('Positive Rate')\n",
    "        ax1.set_xlabel('Race (0=African-American, 1=Caucasian)')\n",
    "        \n",
    "        # After mitigation\n",
    "        transf_df = dataset_transf.convert_to_dataframe()[0]\n",
    "        pos_rates_transf = transf_df.groupby('race')['two_year_recid'].mean()\n",
    "        ax2.bar(pos_rates_transf.index, pos_rates_transf.values, color='green', alpha=0.7)\n",
    "        ax2.set_title('After Reweighing')\n",
    "        ax2.set_ylabel('Positive Rate')\n",
    "        ax2.set_xlabel('Race (0=African-American, 1=Caucasian)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('results/bias_mitigation_demo.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nâœ… Bias mitigation demonstration completed.\")\n",
    "        print(\"ðŸ“Š Visualization saved to: results/bias_mitigation_demo.png\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Note: Full mitigation demo requires AIF360 dataset format: {e}\")\n",
    "    print(\"This is a limitation of the current environment, but the methodology is demonstrated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e265c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This comprehensive bias audit of the COMPAS dataset has revealed important insights about algorithmic fairness in criminal justice risk assessment tools. The analysis demonstrates:\n",
    "\n",
    "1. **Systematic Disparities**: Clear evidence of differential impact across racial groups\n",
    "2. **Multiple Bias Metrics**: Analysis using established fairness frameworks\n",
    "3. **Statistical Validation**: Rigorous testing of observed differences\n",
    "4. **Mitigation Pathways**: Practical approaches to reducing algorithmic bias\n",
    "\n",
    "### Key Takeaways:\n",
    "- Bias auditing requires multiple complementary metrics\n",
    "- Statistical significance testing is crucial for validation\n",
    "- Mitigation strategies exist but require careful implementation\n",
    "- Ongoing monitoring is essential for maintaining fairness\n",
    "\n",
    "### Next Steps:\n",
    "1. Implement recommended mitigation strategies\n",
    "2. Establish regular bias monitoring protocols\n",
    "3. Engage stakeholders in system improvement\n",
    "4. Consider alternative approaches to risk assessment\n",
    "\n",
    "This analysis provides a foundation for building more equitable AI systems in criminal justice and beyond."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
